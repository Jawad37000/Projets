tab <- x  |>
metrics(truth = Level,
estimate = c(.pred_class),
estimator = estim)
accuracy_value <- tab |>
filter(.metric == "accuracy") |>
pull(.estimate)
error <- tibble(
.metric = "error",
.estimator = estim,
.estimate = 1 - accuracy_value
)
tab <- bind_rows(tab, error)
tab$.estimate <- tab$.estimate |> round(3) |> pourcent()
tab
}
tab_res <- function(x, estim) {
metrics <- metric_set(accuracy, f_meas, recall, precision, spec)
auc <- metric_set(roc_auc)
rocauc <- x |> collect_predictions() |>
roc_auc(truth = Level,
.pred_High, .pred_Low, .pred_Medium,
estimator = "macro")
tab <- x |> collect_predictions() |>
metrics(truth = Level,
estimate = c(.pred_class),
estimator = estim) |>
add_row(rocauc)
accuracy_value <- tab |>
filter(.metric == "accuracy") |>
pull(.estimate)
error <- tibble(
.metric = "error",
.estimator = estim,
.estimate = 1 - accuracy_value
)
tab <- bind_rows(tab, error)
tab$.estimate <- tab$.estimate |> round(3) |> pourcent()
tab
}
matconf <- function(x){
conf <- x |> collect_predictions()
conftab <- table(conf$.pred_class,conf$Level)
conftab_dt <- as.data.frame(conftab)
colnames(conftab_dt) <- c("prediction", "truth", "n")
conftab_dt$prediction <- factor(conftab_dt$prediction,levels = c("Medium","Low","High"))
ggplot(conftab_dt, aes(x = truth, y = prediction, fill = n)) +
geom_tile(color = "white") +
scale_fill_gradient(low = "white", high = "darkgreen") +
geom_text(aes(label = n), color = "black", size = 5) +
labs(x = "Vérité",
y = "Prédiction",
fill = "Nombre d'observations") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none",
panel.background = element_blank(),
plot.background = element_blank())
}
tree_best[,1:2] |>  kableset()
tree_tune |> show_best()
#| message: false
set.seed(123) # Pour la reproductibilité
library(skimr)
library(tidyverse)
library(tidymodels)
library(tidy.outliers)
library(knitr)
library(kableExtra)
library(patchwork)
library(naniar)
library(FactoMineR)
library(factoextra)
library(gt)
library(vip)
library(discrim)
library(naivebayes)
library(doParallel)
data <- read.table("data/cancer.csv",
header = T,
sep = ",",
stringsAsFactors = T
)[-c(1,2,4)]
datasum <- data |> skim()
datasum[,c(1,2,3,7,15,8,9)] |> kableset() |>  scroll_box(width = "100%", height = "600px") |> column_spec(2, bold = TRUE, background = "black", color = "white")
dfpivot <- data |>
pivot_longer(cols = -Level, names_to = "Variables", values_to = "Valeurs")
dfpivot |>
ggplot(aes(x = Level, y = Valeurs)) +
geom_boxplot(aes(fill = Level)) +
facet_wrap(~ Variables, scales = "free", ncol = 4) +
theme(panel.spacing = unit(1, "lines"),
plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
axis.title = element_text(size = 16),
axis.text = element_text(size = 14),
strip.text = element_text(size = 14),
legend.text = element_text(size = 14),
legend.title = element_text(size = 16),
panel.grid = element_blank(),
plot.background = element_rect(fill = "lightgrey"),
panel.background = element_rect(fill = "lightyellow"),
legend.background = element_rect(fill= "lightgrey") )+
ggtitle("Boxplot des variables en fonctions  du niveau de risque")
data |>
pivot_longer(cols = -c(Level),names_to = "Variables", values_to = "Valeurs") |>
ggplot(aes(x = Valeurs, fill = Level)) +
geom_histogram(bins = 30, alpha = 0.7, position = "identity") +
facet_wrap(~Variables, scales = "free") +
theme(panel.spacing = unit(1, "lines"),
plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
axis.title = element_text(size = 16),
axis.text = element_text(size = 14),
strip.text = element_text(size = 14),
legend.text = element_text(size = 14),
legend.title = element_text(size = 16),
panel.grid = element_blank(),
plot.background = element_rect(fill = "lightgrey"),
panel.background = element_rect(fill = "lightyellow"),
legend.background = element_rect(fill= "lightgrey"))+
ggtitle("Distribution des variables en fonctions  du niveau de risque")
dfpivot <- data |>
pivot_longer(cols = -Level, names_to = "Variables", values_to = "Valeurs")
dfpivot |>
ggplot(aes(x = Level, y = Valeurs)) +
geom_boxplot(aes(fill = Level)) +
facet_wrap(~ Variables, scales = "free", ncol = 4) +
theme(panel.spacing = unit(1, "lines"),
plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
axis.title = element_text(size = 16),
axis.text = element_text(size = 14),
strip.text = element_text(size = 14),
legend.text = element_text(size = 14),
legend.title = element_text(size = 16),
panel.grid = element_blank(),
plot.background = element_rect(fill = "lightgrey"),
panel.background = element_rect(fill = "lightyellow"),
legend.background = element_rect(fill= "lightgrey") )+
ggtitle("Boxplot des variables en fonctions  du niveau de risque")
corrplot::corrplot(cor(data[,-23]),
method = "color",
tl.cex =1.5, tl.srt = 45,
col = colorRampPalette(c("blue", "white", "red"))(200),
type = "upper",
diag = FALSE,
tl.col = "black")
gg_miss_var_cumsum(data)
table(data$Level) |> t() |> kablesetconf() |>
row_spec(0, font_size = 14, extra_css = "border: 0px;")|>
row_spec(1, font_size = 40, extra_css = "background-color: lightblue;
border-radius: 10px; color: black;")
resACP <- PCA(data, quali.sup = "Level",graph = FALSE)
fviz_pca_var(resACP,
col.var = "steelblue",
col.quali.sup = "red",
repel = TRUE,
labelsize = 6,axes = 1:2) +
theme(text = element_text(size = 14)) + ggtitle("")
fviz_pca_ind(resACP, labelsize = 0, habillage = "Level") + theme(panel.grid = element_blank()) + ggtitle("")
rec <- recipe(Level ~ ., data = data_train)|>
step_corr(all_numeric_predictors(), threshold = tune()) |>
step_outliers_maha(all_numeric(), -all_outcomes()) |>
step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) |>
step_outliers_remove(contains(r"(.outliers)"),score_dropout = tune("dropout"),aggregation_function = "mean")
split_data <- initial_split(data, prop = 0.75, strata = Level)
data_train <- training(split_data)
data_test <- testing(split_data)
data_cv <- vfold_cv(data_train)
rec <- recipe(Level ~ ., data = data_train)|>
step_corr(all_numeric_predictors(), threshold = tune()) |>
step_outliers_maha(all_numeric(), -all_outcomes()) |>
step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) |>
step_outliers_remove(contains(r"(.outliers)"),score_dropout = tune("dropout"),aggregation_function = "mean")
rec <- recipe(Level ~ ., data = data_train)|>
step_corr(all_numeric_predictors(), threshold = 0.42) |>
step_outliers_maha(all_numeric(), -all_outcomes()) |>
step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) |>
step_outliers_remove(contains(r"(.outliers)"),score_dropout = 0.75,aggregation_function = "mean")
lda_mod <- discrim_linear() |>
set_mode("classification") |>
set_engine("MASS")
lda_wk <- workflow() |>
add_model(lda_mod) |>
add_recipe(rec)
lda_fit <- lda_wk |>  last_fit(split_data)
lda_fit |> tab_res("macro")
lda_fit |> collect_predictions() |> roc_curve(Level,.pred_High,.pred_Low,.pred_Medium) |> autoplot() + theme(panel.grid = element_blank())
lda_fit |> matconf()
knn_mod <- nearest_neighbor() |>
set_mode("classification") |>
set_engine("kknn") |>
set_args(neighbors = tune())
knn_wk <- workflow() |>
add_model(knn_mod) |>
add_recipe(rec)
kgrid <- grid_regular(neighbors(),levels = 20)
knn_tune <- tune_grid(knn_wk,
grid = kgrid,
resamples = data_cv,
metrics = metric_set(recall)
)
knn_tune <- tune_grid(knn_wk,
grid = kgrid,
resamples = data_cv,
metrics = metric_set(recall)
)
knnbest <- knn_tune |> select_best(metric = "recall")
knn_fit <- knn_wk |> finalize_workflow(knnbest) |> last_fit(split_data)
knn_tune |> autoplot()
knn_fit |> tab_res("macro")
knn_fit |> collect_predictions() |> roc_curve(Level,.pred_High,.pred_Low,.pred_Medium) |> autoplot() + theme(panel.grid = element_blank())
knn_fit |> matconf()
split_data <- initial_split(data, prop = 0.75, strata = Level)
data_train <- training(split_data)
data_cv <- vfold_cv(data_train)
rec <- recipe(Level ~ ., data = data_train)|>
step_corr(all_numeric_predictors(), threshold = 0.42) |>
step_outliers_maha(all_numeric(), -all_outcomes()) |>
step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) |>
step_outliers_remove(contains(r"(.outliers)"),score_dropout = 0.75,aggregation_function = "mean")
lda_mod <- discrim_linear() |>
set_mode("classification") |>
set_engine("MASS")
lda_wk <- workflow() |>
add_model(lda_mod) |>
add_recipe(rec)
lda_fit <- lda_wk |>  last_fit(split_data)
lda_fit |> tab_res("macro")
lda_fit |> collect_predictions() |> roc_curve(Level,.pred_High,.pred_Low,.pred_Medium) |> autoplot() + theme(panel.grid = element_blank())
lda_fit |> matconf()
knn_mod <- nearest_neighbor() |>
set_mode("classification") |>
set_engine("kknn") |>
set_args(neighbors = tune())
knn_wk <- workflow() |>
add_model(knn_mod) |>
add_recipe(rec)
kgrid <- grid_regular(neighbors(),levels = 20)
knn_tune <- tune_grid(knn_wk,
grid = kgrid,
resamples = data_cv,
metrics = metric_set(recall)
)
knn_tune <- tune_grid(knn_wk,
grid = kgrid,
resamples = data_cv,
metrics = metric_set(recall)
)
knnbest <- knn_tune |> select_best(metric = "recall")
knn_fit <- knn_wk |> finalize_workflow(knnbest) |> last_fit(split_data)
knn_tune |> autoplot()
knn_fit |> tab_res("macro")
knn_fit |> collect_predictions() |> roc_curve(Level,.pred_High,.pred_Low,.pred_Medium) |> autoplot() + theme(panel.grid = element_blank())
knn_fit |> matconf()
anova_result <- aov(data$Age ~ data$Level, data = data)
summary(anova_result)
results <- data.frame(Variable = character(), p_value = numeric(), stringsAsFactors = FALSE)
for (var in names(data)[-ncol(data)]) {
if (is.numeric(data[[var]])) {
anova_result <- aov(data[[var]] ~ data[[ncol(data)]], data = data)
p_value <- summary(anova_result)[[1]]["Pr(>F)"][1]
results <- rbind(results, data.frame(Variable = var, p_value = p_value))
}
}
results
View(results)
results <- data.frame(Variable = character(), Df = numeric(), Sum_Sq = numeric(),
Mean_Sq = numeric(), F_value = numeric(), p_value = numeric(),
stringsAsFactors = FALSE)
for (var in names(data)[-ncol(data)]) {
if (is.numeric(data[[var]])) {
anova_result <- aov(data[[var]] ~ data[[ncol(data)]], data = data)
summary_result <- summary(anova_result)[[1]]
# Extraire les informations sans la ligne des résidus
results <- rbind(results, data.frame(
Variable = var,
Df = summary_result[1, "Df"],
Sum_Sq = summary_result[1, "Sum Sq"],
Mean_Sq = summary_result[1, "Mean Sq"],
F_value = summary_result[1, "F value"],
p_value = summary_result[1, "Pr(>F)"]
))
}
}
print(results)
for (var in names(data)[-ncol(data)]) {
if (is.numeric(data[[var]])) {
anova_result <- aov(data[[var]] ~ data[[ncol(data)]], data = data)
summary_result <- summary(anova_result)[[1]]
# Extraire les informations sans la ligne des résidus
results <- rbind(results, data.frame(
Variable = var,
Df = summary_result[1, "Df"],
Sum_Sq = summary_result[1, "Sum Sq"],
Mean_Sq = summary_result[1, "Mean Sq"],
F_value = summary_result[1, "F value"],
p_value = summary_result[1, "Pr(>F)"]
))
}
}
results
for (var in names(data)[-ncol(data)]) {
if (is.numeric(data[[var]])) {
anova_result <- aov(data[[var]] ~ data[[ncol(data)]], data = data)
summary_result <- summary(anova_result)[[1]]
# Extraire les informations sans la ligne des résidus
results <- rbind(results, data.frame(
Variable = var,
Df = summary_result[1, "Df"],
Sum_Sq = summary_result[1, "Sum Sq"],
Mean_Sq = summary_result[1, "Mean Sq"],
F_value = summary_result[1, "F value"],
p_value = summary_result[1, "Pr(>F)"]
))
}
}
results
results <- data.frame(Variable = character(), Df = numeric(), F_value = numeric(), p_value = numeric(),
stringsAsFactors = FALSE)
for (var in names(data)[-ncol(data)]) {
if (is.numeric(data[[var]])) {
anova_result <- aov(data[[var]] ~ data[[ncol(data)]], data = data)
summary_result <- summary(anova_result)[[1]]
results <- rbind(results, data.frame(
Variable = var,
Df = summary_result[1, "Df"],
F_value = summary_result[1, "F value"],
p_value = summary_result[1, "Pr(>F)"]
))
}
}
results
library(DT)
results |> datatable()
results |>   datatable(
options = list(
pageLength = 5,
autoWidth = TRUE,
scrollX = TRUE,
dom = 'Bfrtip'
)
)
results |>   datatable(
options = list(
pageLength = 5,
scrollX = TRUE,
dom = 'Bfrtip'
)
)
results |>   datatable(
options = list(
pageLength = 5,
scrollX = TRUE,
dom = 'Bfrtip'
)
) |> formatStyle('Variable', fontWeight = 'bold')
results |>   datatable(
options = list(
pageLength = 5,
scrollX = TRUE,
dom = 'Bfrtip'
)
) |> formatStyle('Variable', fontWeight = 'bold') |>
formatStyle('p_value',
backgroundColor = styleInterval(0.05, c('lightcoral', 'lightgreen')),
fontWeight = styleInterval(0.05, c('bold', 'normal')))
load("modèle/tune/qda_tune1.RData")
load("modèle/tune/qda_tune2.RData")
load("modèle/fit/qda_fit.RData")
tabmetqda <- qda_tune2 |> show_best(metric = "recall")
tabmetqda[,-1]  |> kableset()  |> add_header_above(c("Meilleurs paramêtres"=7))
tabmetqda[,1]  |> kableset()  |> add_header_above(c("Meilleurs paramêtres"=7))
tabmetqda[,1]  |> kableset()
tabmetqda[,-1]  |> kableset()
tabmetqda[,1:6]  |> kableset()  |> add_header_above(c("Meilleurs paramêtres"=7))
tabmetqda[,1:6]  |> kableset()  |> add_header_above(c("Meilleurs paramêtres"=6))
load("modèle/tune/rf_tune.RData")
load("modèle/fit/rf_fit.RData")
load("modèle/tune/rf_tune.RData")
load("modèle/tune/qda_tune.RData")
setwd("C:/Users/affre/Desktop/projetS2classificationsuper/Dossier final")
load("modèle/tune/rf_tune.RData")
load("modèle/fit/rf_fit.RData")
rf_fit |>
collect_predictions() |>
roc_curve(Level, .pred_High, .pred_Low, .pred_Medium) |>
ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
labs(,
x = "1 - Spécificité",
y = "Sensibilité",
color = "Classe"
) +
theme_minimal() +
theme(
panel.grid = element_blank(),
text = element_text(size = 12)
)
library(skimr)
library(DT)
library(tidyverse)
library(tidymodels)
library(tidy.outliers)
library(knitr)
library(kableExtra)
library(patchwork)
library(naniar)
library(FactoMineR)
library(factoextra)
library(vip)
set.seed(123)
rf_tune |> autoplot()
final_model <-  tree_fit |> extract_workflow() |> fit(data)
varimportance <- final_model |> pull_workflow_fit()
load("modèle/tune/tree_tune.RData")
load("modèle/fit/tree_fit.RData")
final_model <-  tree_fit |> extract_workflow() |> fit(data)
varimportance <- final_model |> pull_workflow_fit()
vardf <- data.frame(
Variable = names(varimportance$fit$variable.importance),
Importance = varimportance$fit$variable.importance
)
ggplot(vardf, aes(x = reorder(Variable, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs( x = "", y = "") +
theme_minimal()
final_rf_model <-  rf_fit |> extract_workflow() |> fit(data)
varimportance <- final_rf_model$fit$fit$fit$importance
varrfdf <- data.frame(
Variable = rownames(varimportance),
Importance = varimportance[, 5]
)
ggplot(varrfdf, aes(x = reorder(Variable, Importance), y = Importance)) +
geom_bar(stat = "identity", fill = "lightblue") +
coord_flip() +
labs( x = "", y = "") +
theme_minimal()
load("modèle/tune/boost_tune.RData")
load("modèle/fit/boost_fit.RData")
boost_tune_tbl <- boost_tune |> collect_metrics()
ggplot(boost_tune_tbl, aes(x = mtry, y = mean, color = as.factor(min_n), group = min_n)) +
geom_line() +
geom_point() +
facet_wrap(~ tree_depth) +
labs(
title = "Peboostormance en fonction de mtry et min_n pour chaque tree_depth",
x = "mtry",
y = "Recall (moyenne)",
color = "min_n"
) +
theme_minimal()
library(skimr)
library(DT)
library(tidyverse)
library(tidymodels)
library(tidy.outliers)
library(knitr)
library(kableExtra)
library(patchwork)
library(naniar)
library(FactoMineR)
library(factoextra)
library(vip)
set.seed(123)
boost_tune_tbl <- boost_tune |> collect_metrics()
ggplot(boost_tune_tbl, aes(x = mtry, y = mean, color = as.factor(min_n), group = min_n)) +
geom_line() +
geom_point() +
facet_wrap(~ tree_depth) +
labs(
title = "Peboostormance en fonction de mtry et min_n pour chaque tree_depth",
x = "mtry",
y = "Recall (moyenne)",
color = "min_n"
) +
theme_minimal()
boost_tune |> autoplot()
boost_fit |> tab_res("macro") |> xtable(caption = "Résultats")
boost_fit |> tab_res("macro") |> xtable(caption = "Résultats")
library(xtable)
boost_fit |> tab_res("macro") |> xtable(caption = "Résultats")
boost_fit |>
collect_predictions() |>
roc_curve(Level, .pred_High, .pred_Low, .pred_Medium) |>
ggplot(aes(x = 1 - specificity, y = sensitivity, color = .level)) +
geom_line(size = 1.2) +
geom_abline(linetype = "dashed", color = "gray") +
labs(,
x = "1 - Spécificité",
y = "Sensibilité",
color = "Classe"
) +
theme_minimal() +
theme(
panel.grid = element_blank(),
text = element_text(size = 12)
)
boost_confmat <- boost_fit |>
collect_predictions()
boost_confmat$.pred_class <- factor(boost_confmat$.pred_class, levels = c("High","Medium","Low"))
boost_confmat$Level <- factor(boost_confmat$Level, levels = c("High","Medium","Low"))
boost_confmat |>
conf_mat(truth = Level, estimate = .pred_class) |>
autoplot(type = "heatmap")
vbvarimportance <- extract_fit_parsnip(boost_fit$.workflow[[1]])
importance_boosting <- xgboost::xgb.importance(model = vbvarimportance$fit)
vip(vbvarimportance)
boost_tune |> autoplot()
boost_fit |> tab_res("macro") |> xtable(caption = "Résultats")
boost_fit |> tab_res("macro") |> xtable(caption = "Résultats")
Voici un petit recapitulatif des modeles les plus performants.
